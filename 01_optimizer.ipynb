{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Contains code used/modified by fastai_minima author from fastai\n",
    "# Copyright 2019 the fast.ai team.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *\n",
    "from fastcore.basics import Self\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from functools import partial\n",
    "\n",
    "def noop (x=None, *args, **kwargs):\n",
    "    \"Do nothing\"\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from collections import defaultdict\n",
    "\n",
    "from fastcore.basics import merge, range_of, even_mults, GetAttr\n",
    "from fastcore.foundation import L\n",
    "from fastcore.meta import delegates\n",
    "from fastcore.xtras import is_listy\n",
    "\n",
    "from fastai_minima.utils import tensor\n",
    "\n",
    "import torch\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers\n",
    "> Define the general fastai optimizer and variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the actual fastai documentation, you should go to the [Optimizer](docs.fast.ai/optimizer) documentation. These are minimal docs simply to bring in the source code and related tests to ensure that minimal functionality is met"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `_BaseOptimizer_` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class _BaseOptimizer():\n",
    "    \"Common functionality between `Optimizer` and `OptimWrapper`\"\n",
    "    def all_params(self, n=slice(None), with_grad=False):\n",
    "        \"List of param_groups, paramters, and hypers\"\n",
    "        res = L((p,pg,self.state[p],hyper) for pg,hyper in zip(self.param_lists[n],self.hypers[n]) for p in pg)\n",
    "        return L(o for o in res if hasattr(o[0], 'grad') and o[0].grad is not None) if with_grad else res\n",
    "\n",
    "    def _set_require_grad(self, rg, p,pg,state,h): p.requires_grad_(rg or state.get('force_train', False))\n",
    "    def freeze_to(self, n):\n",
    "        \"Freeze parameter groups up to `n`\"\n",
    "        self.frozen_idx = n if n >= 0 else len(self.param_lists) + n\n",
    "        if self.frozen_idx >= len(self.param_lists):\n",
    "            warn(f\"Freezing {self.frozen_idx} groups; model has {len(self.param_lists)}; whole model is frozen.\")\n",
    "        for o in self.all_params(slice(n, None)): self._set_require_grad(True,  *o)\n",
    "        for o in self.all_params(slice(None, n)): self._set_require_grad(False, *o)\n",
    "\n",
    "    def freeze(self):\n",
    "        \"Freeze up to last parameter group\"\n",
    "        assert(len(self.param_lists)>1)\n",
    "        self.freeze_to(-1)\n",
    "\n",
    "    def set_freeze(self, n, rg, ignore_force_train=False):\n",
    "        \"Set `rg` for parameter group `n` only\"\n",
    "        for p in self.param_lists[n]: p.requires_grad_(rg or (state.get('force_train', False) and not ignore_force_train))\n",
    "\n",
    "    def unfreeze(self): \n",
    "        \"Unfreeze the entire model\"\n",
    "        self.freeze_to(0)\n",
    "    def set_hypers(self, **kwargs):\n",
    "        \"Apply `set_hyper` for all `kwargs`\"\n",
    "        L(kwargs.items()).starmap(self.set_hyper)\n",
    "    def _set_hyper(self, k, v):\n",
    "        for v_,h in zip(v, self.hypers): h[k] = v_\n",
    "\n",
    "    def set_hyper(self, k, v):\n",
    "        \"Set the value(s) in `v` for hyper-paramter `k`\"\n",
    "        if isinstance(v, slice):\n",
    "            if v.start: v = even_mults(v.start, v.stop, len(self.param_lists))\n",
    "            else: v = [v.stop/10]*(len(self.param_lists)-1) + [v.stop]\n",
    "        v = L(v, use_list=None)\n",
    "        if len(v)==1: v = v*len(self.param_lists)\n",
    "        assert len(v) == len(self.hypers), f\"Trying to set {len(v)} values for {k} but there are {len(self.param_lists)} parameter groups.\"\n",
    "        self._set_hyper(k, v)\n",
    "\n",
    "    @property\n",
    "    def param_groups(self): return [{**{'params': pg}, **hp} for pg,hp in zip(self.param_lists, self.hypers)]\n",
    "    @param_groups.setter\n",
    "    def param_groups(self, v):\n",
    "        for pg,v_ in zip(self.param_lists,v): pg = v_['params']\n",
    "        for hyper,v_ in zip(self.hypers,v):\n",
    "            for k,t in v_.items():\n",
    "                if k != 'params': hyper[k] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _update(state, new=None):\n",
    "    if new is None: return state\n",
    "    if isinstance(new, dict): state.update(new)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `OptimWrapper` - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def detuplify_pg(d):\n",
    "    res = {}\n",
    "    for k,v in d.items():\n",
    "        if k == 'params': continue\n",
    "        if is_listy(v): res.update(**{f'{k}__{i}': v_ for i,v_ in enumerate(v)})\n",
    "        else: res[k] = v\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "tst = {'lr': 1e-2, 'mom': 0.9, 'params':[0,1,2]}\n",
    "test_eq(detuplify_pg(tst), {'lr': 1e-2, 'mom': 0.9})\n",
    "tst = {'lr': 1e-2, 'betas': (0.9,0.999), 'params':[0,1,2]}\n",
    "test_eq(detuplify_pg(tst), {'lr': 1e-2, 'betas__0': 0.9, 'betas__1': 0.999})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def set_item_pg(pg, k, v):\n",
    "    if '__' not in k: pg[k] = v\n",
    "    else:\n",
    "        name,idx = k.split('__')\n",
    "        pg[name] = tuple(v if i==int(idx) else pg[name][i] for i in range_of(pg[name]))\n",
    "    return pg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "tst = {'lr': 1e-2, 'mom': 0.9, 'params':[0,1,2]}\n",
    "test_eq(set_item_pg(tst, 'lr', 1e-3), {'lr': 1e-3, 'mom': 0.9, 'params':[0,1,2]})\n",
    "tst = {'lr': 1e-2, 'betas': (0.9,0.999), 'params':[0,1,2]}\n",
    "test_eq(set_item_pg(tst, 'betas__0', 0.95), {'lr': 1e-2, 'betas': (0.95,0.999), 'params':[0,1,2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "pytorch_hp_map = {'momentum': 'mom', 'weight_decay': 'wd', 'alpha': 'sqr_mom', 'betas__0': 'mom', 'betas__1': 'sqr_mom'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class OptimWrapper(_BaseOptimizer, GetAttr):\n",
    "    _xtra=['zero_grad', 'step', 'state_dict', 'load_state_dict']\n",
    "    _default='opt'\n",
    "    def __init__(self, opt, hp_map=None):\n",
    "        self.opt = opt\n",
    "        if hp_map is None: hp_map = pytorch_hp_map\n",
    "        self.fwd_map = {k: hp_map[k] if k in hp_map else k for k in detuplify_pg(opt.param_groups[0]).keys()}\n",
    "        self.bwd_map = {v:k for k,v in self.fwd_map.items()}\n",
    "        self.state = defaultdict(dict, {})\n",
    "        self.frozen_idx = 0\n",
    "\n",
    "    @property\n",
    "    def hypers(self):\n",
    "        return [{self.fwd_map[k]:v for k,v in detuplify_pg(pg).items() if k != 'params'} for pg in self.opt.param_groups]\n",
    "\n",
    "    def _set_hyper(self, k, v):\n",
    "        for pg,v_ in zip(self.opt.param_groups,v): pg = set_item_pg(pg, self.bwd_map[k], v_)\n",
    "\n",
    "    def clear_state(self): self.opt.state = defaultdict(dict, {})\n",
    "\n",
    "    @property\n",
    "    def param_lists(self): return [pg['params'] for pg in self.opt.param_groups]\n",
    "    @param_lists.setter\n",
    "    def param_lists(self, v):\n",
    "        for pg,v_ in zip(self.opt.param_groups,v): pg['params'] = v_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `OptimWrapper` Examples\n",
    "\n",
    "Below are some examples with `OptimWrapper` with Pytorch optimizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "@delegates(optim.Adam)\n",
    "def Adam(params, **kwargs): \n",
    "    \"Convience function to make an Adam optimizer compatable with `Learner`\"\n",
    "    return OptimWrapper(optim.Adam(params, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "@delegates(optim.SGD)\n",
    "def SGD(params, **kwargs):\n",
    "    \"Convience function to make a SGD optimizer compatable with `Learner`\"\n",
    "    return OptimWrapper(optim.SGD(params, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "sgd = SGD([tensor([1,2,3])], lr=1e-3, momentum=0.9, weight_decay=1e-2)\n",
    "tst_sgd = OptimWrapper(torch.optim.SGD([tensor([1,2,3])], lr=1e-3, momentum=0.9, weight_decay=1e-2))\n",
    "#Access to param_groups\n",
    "test_eq(tst_sgd.param_lists, sgd.param_lists)\n",
    "#Set param_groups\n",
    "tst_sgd.param_lists = [[tensor([4,5,6])]]\n",
    "test_eq(tst_sgd.opt.param_groups[0]['params'], [tensor(4,5,6)])\n",
    "#Access to hypers\n",
    "test_eq(tst_sgd.hypers, [{**sgd.hypers[0], 'dampening': 0., 'nesterov': False}])\n",
    "#Set hypers\n",
    "tst_sgd.set_hyper('mom', 0.95)\n",
    "test_eq(tst_sgd.opt.param_groups[0]['momentum'], 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "sgd = SGD([tensor([1,2,3])], lr=1e-3, momentum=0.9, weight_decay=1e-2)\n",
    "tst_sgd = OptimWrapper(torch.optim.SGD([tensor([1,2,3])], lr=1e-3, momentum=0.9, weight_decay=1e-2))\n",
    "#Access to param_groups\n",
    "test_eq(tst_sgd.param_lists, sgd.param_lists)\n",
    "#Set param_groups\n",
    "tst_sgd.param_lists = [[tensor([4,5,6])]]\n",
    "test_eq(tst_sgd.opt.param_groups[0]['params'], [tensor(4,5,6)])\n",
    "#Access to hypers\n",
    "test_eq(tst_sgd.hypers, [{**sgd.hypers[0], 'dampening': 0., 'nesterov': False}])\n",
    "#Set hypers\n",
    "tst_sgd.set_hyper('mom', 0.95)\n",
    "test_eq(tst_sgd.opt.param_groups[0]['momentum'], 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "tst_sgd = OptimWrapper(torch.optim.SGD([{'params': [tensor([1,2,3])], 'lr': 1e-3}, \n",
    "                                        {'params': [tensor([4,5,6])], 'lr': 1e-2}], momentum=0.9, weight_decay=1e-2))\n",
    "sgd = SGD([{'params': [tensor([1,2,3])], 'lr': 1e-3}, \n",
    "                                        {'params': [tensor([4,5,6])], 'lr': 1e-2}], momentum=0.9, weight_decay=1e-2)\n",
    "#Access to param_groups\n",
    "test_eq(tst_sgd.param_lists, sgd.param_lists)\n",
    "#Set param_groups\n",
    "tst_sgd.param_lists = [[tensor([4,5,6])], [tensor([1,2,3])]]\n",
    "test_eq(tst_sgd.opt.param_groups[0]['params'], [tensor(4,5,6)])\n",
    "test_eq(tst_sgd.opt.param_groups[1]['params'], [tensor(1,2,3)])\n",
    "#Access to hypers\n",
    "test_eq(tst_sgd.hypers, [{**sgd.hypers[i], 'dampening': 0., 'nesterov': False} for i in range(2)])\n",
    "#Set hypers\n",
    "tst_sgd.set_hyper('mom', 0.95)\n",
    "test_eq([pg['momentum'] for pg in tst_sgd.opt.param_groups], [0.95,0.95])\n",
    "tst_sgd.set_hyper('lr', [1e-4,1e-3])\n",
    "test_eq([pg['lr'] for pg in tst_sgd.opt.param_groups], [1e-4,1e-3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differential Learning Rates and Groups with Pytorch Optimizers\n",
    "\n",
    "Out of the box, `OptimWrapper` is not able to utilize param groups and differential learning rates like `fastai` has. Below contains the necissary helper functions needed, as well as a tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def params(m):\n",
    "    \"Return all parameters of `m`\"\n",
    "    return [p for p in m.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch import nn\n",
    "def convert_params(o:list) -> list:\n",
    "    \"\"\"\n",
    "    Converts `o` into Pytorch-compatable param groups\n",
    "    \n",
    "    `o` should be a set of layer-groups that should be split in the optimizer\n",
    "    \n",
    "    Example:\n",
    "    \n",
    "    ```python\n",
    "    def splitter(m): return convert_params([[m.a], [m.b]])\n",
    "    ```\n",
    "    \n",
    "    Where `m` is a model defined as:\n",
    "    \n",
    "    ```python\n",
    "    class RegModel(Module):\n",
    "      def __init__(self): self.a,self.b = nn.Parameter(torch.randn(1)),nn.Parameter(torch.randn(1))\n",
    "      def forward(self, x): return x*self.a + self.b\n",
    "    ```\n",
    "    \"\"\"\n",
    "    if not isinstance(o[0], dict):\n",
    "        splitter = []\n",
    "        for group in o:\n",
    "            if not isinstance(group[0], nn.parameter.Parameter):\n",
    "                group = L(group).map(params)\n",
    "            splitter.append({'params':group})\n",
    "        return splitter\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mock_train(m, x, y, opt):\n",
    "    m.train()\n",
    "    for i in range(0, 100, 25):\n",
    "        z = m(x[i:i+25])\n",
    "        loss = F.mse_loss(z, y[i:i+25])\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
